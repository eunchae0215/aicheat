from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.agents import initialize_agent, Tool
from langchain.chat_models import ChatOpenAI

# 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ ë° ì„ë² ë”© ì €ì¥
loader = TextLoader("docs.txt")
docs = loader.load()
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = splitter.split_documents(docs)

embedding = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(splits, embedding)
retriever = vectorstore.as_retriever()

# 2ë‹¨ê³„: RAG ì²´ì¸ êµ¬ì„±
llm = ChatOpenAI(temperature=0)
rag_chain = ConversationalRetrievalChain.from_llm(llm, retriever)

# 3ë‹¨ê³„: Agent ì²´ì¸ êµ¬ì„±
retriever_tool = Tool(
    name="document_retriever",
    func=retriever.get_relevant_documents,
    description="Fetch documents relevant to user question from the knowledge base."
)

agent = initialize_agent([retriever_tool], llm, agent_type="zero-shot-react-description", verbose=True)

# 4ë‹¨ê³„: ë¹„êµ ì§ˆë¬¸ ë° ì‘ë‹µ
questions = [
    "What are the key components of LangChain?",
    "How is context handled differently in rag versus chat agents?"
]

chat_history = []
for q in questions:
    print(f"\n[ì§ˆë¬¸] {q}\n")
    
    rag_response = rag_chain.run({"question": q, "chat_history": chat_history})
    print(f"ğŸ”· RAG ì‘ë‹µ:\n{rag_response}")
    
    agent_response = agent.run(q)
    print(f"ğŸŸ£ Agent ì‘ë‹µ:\n{agent_response}")
    
    print("\n[ë¶„ì„]")
    print("RAGëŠ” ë¬¸ì„œ ê¸°ë°˜ ì§ì ‘ ì‘ë‹µ ë°©ì‹ìœ¼ë¡œ, contextê°€ ì •í™•íˆ í¬í•¨ë˜ì—ˆì„ ë•Œ ë†’ì€ ì‹ ë¢°ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
    print("AgentëŠ” retriever toolì„ reasoningì— í™œìš©í•´ ë” ê¸´ ì‘ë‹µê³¼ ë„êµ¬ í™œìš© ê¸°ë°˜ ì¶”ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤.")
    print("ë¬¸ì„œ ê¸°ë°˜ ì •ë³´ ì •ë¦¬ëŠ” RAGê°€ ìœ ë¦¬í•˜ê³ , ì™¸ë¶€ ë„êµ¬ í™œìš©ì´ë‚˜ ë³µí•© ì§ˆì˜ëŠ” Agentê°€ ë” ê°•ë ¥í•©ë‹ˆë‹¤.")

