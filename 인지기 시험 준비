# Create a prompt template from the template string
template = "You are an artificial intelligence assistant, answer the question. {question}"
prompt = PromptTemplate.from_template(
    template=template
)

llm = ChatOpenAI(
    model="gpt-4o-mini",
    api_key="<OPENAI_API_TOKEN>"
)

# Create a chain to integrate the prompt template and LLM
llm_chain = prompt | llm

# Invoke the chain on the question
question = "How does LangChain make LLM application development easier?"
print(llm_chain.invoke({"question": question}))

llm = ChatOpenAI(model="gpt-4o-mini", api_key='<OPENAI_API_TOKEN>')

# Create a chat prompt template
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a geography expert that returns the colors present in a country's flag."),
        ("human", "France"),
        ("ai", "blue, white, red"),
        ("human", "{country}")
    ]
)

llm_chain = prompt_template | llm

country = "Japan"
response = llm_chain.invoke({"country": country})
print(response.content)


# Complete the prompt for formatting answers
example_prompt = PromptTemplate.from_template("Question: {question}\n{answer}")

# Create the few-shot prompt
prompt_template = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

prompt = prompt_template.invoke(
    {"input": "What is Jack's favorite technology on DataCamp?"})
print(prompt.text)


prompt_template = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

# Create an OpenAI chat LLM
# Create an OpenAI chat LLM
llm = ChatOpenAI(model="gpt-4o-mini", api_key="<OPENAI_API_TOKEN>")

# Create and invoke the chain
llm_chain = prompt_template | llm
print(llm_chain.invoke({"input": "What is Jack's favorite technology on DataCamp?"}))


# Create a prompt template that takes an input activity
learning_prompt = PromptTemplate(
    input_variables=["activity"],
    template="I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?"
)

# Create a prompt template that places a time constraint on the output
time_prompt = PromptTemplate(
    input_variables=["learning_plan"],
    template="I only have one week. Can you create a plan to help me hit this goal: {learning_plan}."
)

# Invoke the learning_prompt with an activity
print(learning_prompt.invoke({"activity": "play golf"}))


# Define a function to retrieve customer info by name
def retrieve_customer_info(name: str) -> str:
    """Retrieve customer information based on their name."""
    # Filter customers for the customer's name
    customer_info = customers[customers['name'] == name]
    return customer_info.to_string()

# Call the function on Peak Performance Co.
print(retrieve_customer_info("Peak Performance Co."))


# Convert the retrieve_customer_info function into a tool
from langchain.agents import tool

@tool
def retrieve_customer_info(name: str) -> str:
    """Retrieve customer information based on their name."""
    customer_info = customers[customers['name'] == name]
    return customer_info.to_string()

# Print the tool's arguments
print(retrieve_customer_info.args)


@tool
def retrieve_customer_info(name: str) -> str:
    """Retrieve customer information based on their name."""
    customer_info = customers[customers['name'] == name]
    return customer_info.to_string()

# Create a ReAct agent
agent = create_react_agent(llm, [retrieve_customer_info])

# Invoke the agent on the input
messages = agent.invoke({"messages": [("human", "Create a summary of our customer: Peak Performance Co.")]})
print(messages['messages'][-1].content)





# Import library
from langchain_community.document_loaders import PyPDFLoader

# Create a document loader for rag_vs_fine_tuning.pdf
loader = PyPDFLoader("rag_vs_fine_tuning.pdf")

# Load the document
data = loader.load()
print(data[0])












# Import library
from langchain_community.document_loaders.csv_loader import CSVLoader

# Create a document loader for fifa_countries_audience.csv
loader = CSVLoader(file_path="fifa_countries_audience.csv")

# Load the document
data = loader.load()
print(data[0])















from langchain_community.document_loaders import UnstructuredHTMLLoader

# Create a document loader for unstructured HTML
loader = UnstructuredHTMLLoader("white_house_executive_order_nov_2023.html")

# Load the document
data = loader.load()

# Print the first document
print(data[0])

# Print the first document's metadata
print(data[0].metadata)










# Import the character splitter
from langchain_text_splitters import CharacterTextSplitter

quote = 'Words are flowing out like endless rain into a paper cup,\nthey slither while they pass,\nthey slip away across the universe.'
chunk_size = 24
chunk_overlap = 10

# Create an instance of the splitter class
splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)

# Split the string and print the chunks
docs = splitter.split_text(quote)
print(docs)
print([len(doc) for doc in docs])









다음 요구사항에 맞는 LangChain RAG 체인을 구성하는 코드를 작성하시오.
요구사항:

문서 파일 rag_vs_fine_tuning.pdf를 불러와 chunk 단위로 분할

text-embedding-3-small 모델을 사용하여 임베딩

Chroma 벡터스토어에 저장

검색된 문서를 사용해 LLM 응답 생성 체인을 만들 것 (k=3)
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough

loader = PyPDFLoader("rag_vs_fine_tuning.pdf")
data = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
docs = splitter.split_documents(data)

embedding = OpenAIEmbeddings(api_key="<OPENAI_API_TOKEN>", model="text-embedding-3-small")
vectorstore = Chroma.from_documents(docs, embedding=embedding)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

prompt = ChatPromptTemplate.from_messages([("human", 
    "Answer the question using the context:\n\nContext:\n{context}\nQuestion:\n{question}")])

llm = ChatOpenAI(api_key="<OPENAI_API_TOKEN>", model="gpt-4o-mini")

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)









 LangChain에서 tool 데코레이터를 사용해 고객 이름을 기반으로 고객 정보를 출력하는 기능을 구현하시오.
조건:

Pandas의 customers DataFrame에서 이름이 일치하는 행만 검색

LangChain Agent에 tool로 등록 가능한 형태로 작성

from langchain.agents import tool

@tool
def retrieve_customer_info(name: str) -> str:
    """Retrieve customer information based on their name."""
    customer_info = customers[customers['name'] == name]
    return customer_info.to_string()















1. RAG(Retrieval-Augmented Generation)의 구조와 동작 방식에 대해 설명하시오.
핵심 키워드 유도: Retriever, LLM, 문서 기반 응답, hallucination 감소 등

2. LangChain의 Retriever와 Agent의 차이점을 설명하시오.
포인트: Retriever는 문서를 찾고, Agent는 도구를 써서 더 복잡한 작업 수행

3. 임베딩(embedding)을 벡터 데이터베이스와 함께 사용할 때의 이점은 무엇인가?
예시: 의미 기반 검색, 유사 문서 클러스터링, 분류 가능성 등

4. LangChain에서 Embedding 모델과 Vectorstore가 함께 사용되는 전체 흐름을 간단히 서술하시오.
입력 → embedding → 벡터 DB 저장 → 질의 시 유사 문서 검색 → LLM 응답 생성







다음 두 코드의 구조적 차이점과 사용 목적의 차이를 설명하시오.

코드 A: RAG 체인 구성
python
복사
편집
retriever = vectorstore.as_retriever()
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)
코드 B: LangChain Agent 구성
python
복사
편집
@tool
def retrieve_info(name: str) -> str:
    return customers[customers['name'] == name].to_string()

agent = create_react_agent(llm, [retrieve_info])
response = agent.invoke({"messages": [("human", "Show info for Peak Inc.")]})
✅ 정답 포인트:
**코드 A (RAG)**는 문서 기반 질의응답을 위한 체인이다. retriever는 관련 문서를 검색해 LLM에게 전달하며, 정적 정보 기반 답변에 적합하다.

**코드 B (Agent)**는 외부 도구 호출이 가능한 방식으로, 사용자의 요청에 따라 직접 함수를 실행하는 등 동적 실행 기능이 포함된다.

Agent는 tool을 통해 직접 계산, 웹 요청, DB 조회 등 액션 수행이 가능하므로 RAG보다 더 복잡한 작업을 처리할 수 있다.




















아래 두 임베딩 처리 방식의 차이점과 장단점을 설명하시오.

코드 A: Embedding + Vector DB (RAG)
python
복사
편집
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(docs, embedding=embeddings)
retriever = vectorstore.as_retriever()
코드 B: Fine-tuning 기반 QA
python
복사
편집
finetuned_model = AutoModelForSequenceClassification.from_pretrained("finetuned-qa-model")
✅ 정답 포인트:
코드 A는 사전 훈련된 임베딩 모델로 문서를 벡터화하고, 벡터 DB를 통해 유사 문서를 검색한다. RAG 방식은 문서 추가/수정이 간편하고 유연함.

코드 B는 모델 자체를 특정 데이터셋에 맞게 파인튜닝한 방식으로, 정확도는 높을 수 있으나 학습 비용이 크고 유연성이 낮다.

일반적으로 자주 변경되는 문서나 FAQ 시스템에는 RAG 방식이 적합하며, 정적인 고정된 질문 세트에는 fine-tuning이 유리할 수 있다.

❓ 문제 3.
다음 두 문서 분할(splitter) 방식의 차이점과 사용 적절성을 설명하시오.

코드 A:
python
복사
편집
splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
코드 B:
python
복사
편집
splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50, separators=["\n\n", "\n", " ", ""])
✅ 정답 포인트:
CharacterTextSplitter는 단순히 고정된 기준으로 문자열을 자르며, 문장 구조와 무관하게 끊어질 수 있다.

RecursiveCharacterTextSplitter는 큰 단위(단락 → 문장 → 단어 → 문자) 순서로 자르며, 의미 단위를 최대한 보존하는 방식이다.

일반적으로 문서 기반 검색용으로는 RecursiveCharacterTextSplitter가 더 자연스럽고 정확한 검색에 유리하다.






